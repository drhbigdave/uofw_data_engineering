{"paragraphs":[{"text":"%spark2.conf\n\nmaster  local[*]\n\n# set driver memrory to 12g\nspark.driver.memory 12g\n\n# set executor memrory 4g\nspark.executor.memory  4g\n","user":"anonymous","dateUpdated":"2019-05-27T22:34:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558996440449_-1555318324","id":"20190527-223400_369025306","dateCreated":"2019-05-27T22:34:00+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3016"},{"text":"%md\n\n#### Step 1: Import in all the columns of the tables hh_demographic, product and transaction_data.\n\n##### ---------------------------------------------------------------------------------------------\n\n#### As transaction_data has both household_key and product_id columns, the import will bring in all of the\n#### columns from transaction_data and drop the 2 mentioned columns/keys from the other 2 tables. ","user":"anonymous","dateUpdated":"2019-05-21T22:21:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Step 1: Import in all the columns of the tables hh_demographic, product and transaction_data.</h4>\n<h5>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</h5>\n<h4>As transaction_data has both household_key and product_id columns, the import will bring in all of the</h4>\n<h4>columns from transaction_data and drop the 2 mentioned columns/keys from the other 2 tables.</h4>\n"}]},"apps":[],"jobName":"paragraph_1558306866102_-1415621176","id":"20190519-230106_1218424392","dateCreated":"2019-05-19T23:01:06+0000","dateStarted":"2019-05-21T22:21:23+0000","dateFinished":"2019-05-21T22:21:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3017"},{"text":"%sh\n\n##### NOTES #####\n# that was hard to find any real info on adding the shell interpreter when it was already installed \n#// in the sandbox \n# the paragraphs after this sqoop import informs the table names and column names here \n# the where $CONDITIONS is a sqoop thing that helps with splitting up over the threads/workers properly\n# had to bump up the shell timeout milliseconds from 6000 to 600000\n# still fighting with directories on the linux VM that between root vs zeppelin am creating but can't destroy \n# deleting directories with new paragraph\n\nsqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver \\\n--connection-manager \"org.apache.sqoop.manager.SQLServerManager\" \\\n--connect \"jdbc:sqlserver://bigdata220.database.windows.net:1433;database=week3\" \\\n--m 4 \\\n--username bigdata \\\n--password \"\" \\\n--target-dir sqoopFull \\\n--as-parquetfile \\\n--query 'Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE $CONDITIONS' \\\n--split-by d.household_key\n\n\n","user":"anonymous","dateUpdated":"2019-06-12T14:53:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n19/05/19 22:50:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\n19/05/19 22:50:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n19/05/19 22:50:12 INFO manager.SqlManager: Using default fetchSize of 1000\n19/05/19 22:50:12 INFO tool.CodeGenTool: Beginning code generation\n19/05/19 22:50:14 INFO manager.SqlManager: Executing SQL statement: Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 0) \n19/05/19 22:50:14 INFO manager.SqlManager: Executing SQL statement: Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 0) \n19/05/19 22:50:14 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hdp/3.0.1.0-187/hadoop-mapreduce\n19/05/19 22:50:20 ERROR orm.CompilationManager: Could not rename /tmp/sqoop-zeppelin/compile/4eb147152a14b1e74c7b729170da6c22/QueryResult.java to /home/zeppelin/./QueryResult.java. Error: Destination '/home/zeppelin/./QueryResult.java' already exists\n19/05/19 22:50:20 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-zeppelin/compile/4eb147152a14b1e74c7b729170da6c22/QueryResult.jar\n19/05/19 22:50:20 INFO mapreduce.ImportJobBase: Beginning query import.\n19/05/19 22:50:22 INFO manager.SqlManager: Executing SQL statement: Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 0) \n19/05/19 22:50:22 INFO manager.SqlManager: Executing SQL statement: Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 0) \n19/05/19 22:50:22 INFO manager.SqlManager: Executing SQL statement: Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 0) \n19/05/19 22:50:23 INFO conf.HiveConf: Found configuration file file:/etc/hive/3.0.1.0-187/0/hive-site.xml\n19/05/19 22:50:26 INFO client.RMProxy: Connecting to ResourceManager at sandbox-hdp.hortonworks.com/172.18.0.3:8050\n19/05/19 22:50:27 INFO client.AHSProxy: Connecting to Application History server at sandbox-hdp.hortonworks.com/172.18.0.3:10200\n19/05/19 22:50:27 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/zeppelin/.staging/job_1558283762263_0003\n19/05/19 22:50:35 INFO db.DBInputFormat: Using read commited transaction isolation\n19/05/19 22:50:35 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(t1.household_key), MAX(t1.household_key) FROM (Select\nd.*,p.MANUFACTURER,p.DEPARTMENT,p.BRAND,p.COMMODITY_DESC,p.SUB_COMMODITY_DESC,p.CURR_SIZE_OF_PRODUCT,h.AGE_DESC,h.MARITAL_STATUS_CODE,h.INCOME_DESC,\nh.HOMEOWNER_DESC,h.HH_COMP_DESC,h.HOUSEHOLD_SIZE_DESC,h.KID_CATEGORY_DESC FROM transaction_data d JOIN product p ON d.PRODUCT_ID = p.PRODUCT_ID LEFT JOIN\n hh_demographic h ON d.household_key = h.household_key WHERE  (1 = 1) ) AS t1\n19/05/19 22:50:50 INFO db.IntegerSplitter: Split size: 624; Num splits: 4 from: 1 to: 2500\n19/05/19 22:50:51 INFO mapreduce.JobSubmitter: number of splits:4\n19/05/19 22:50:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558283762263_0003\n19/05/19 22:50:51 INFO mapreduce.JobSubmitter: Executing with tokens: []\n19/05/19 22:50:52 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.1.0-187/0/resource-types.xml\n19/05/19 22:50:52 INFO impl.YarnClientImpl: Submitted application application_1558283762263_0003\n19/05/19 22:50:52 INFO mapreduce.Job: The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1558283762263_0003/\n19/05/19 22:50:52 INFO mapreduce.Job: Running job: job_1558283762263_0003\n19/05/19 22:51:09 INFO mapreduce.Job: Job job_1558283762263_0003 running in uber mode : false\n19/05/19 22:51:09 INFO mapreduce.Job:  map 0% reduce 0%\n19/05/19 22:52:35 INFO mapreduce.Job:  map 25% reduce 0%\n19/05/19 22:53:45 INFO mapreduce.Job:  map 50% reduce 0%\n19/05/19 22:53:48 INFO mapreduce.Job:  map 75% reduce 0%\n19/05/19 22:54:10 INFO mapreduce.Job:  map 100% reduce 0%\n19/05/19 22:54:10 INFO mapreduce.Job: Job job_1558283762263_0003 completed successfully\n19/05/19 22:54:10 INFO mapreduce.Job: Counters: 32\n\tFile System Counters\n\t\tFILE: Number of bytes read=0\n\t\tFILE: Number of bytes written=997852\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=130200\n\t\tHDFS: Number of bytes written=37966086\n\t\tHDFS: Number of read operations=272\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=40\n\tJob Counters \n\t\tLaunched map tasks=4\n\t\tOther local map tasks=4\n\t\tTotal time spent by all maps in occupied slots (ms)=1935020\n\t\tTotal time spent by all reduces in occupied slots (ms)=0\n\t\tTotal time spent by all map tasks (ms)=483755\n\t\tTotal vcore-milliseconds taken by all map tasks=483755\n\t\tTotal megabyte-milliseconds taken by all map tasks=495365120\n\tMap-Reduce Framework\n\t\tMap input records=2595732\n\t\tMap output records=2595732\n\t\tInput split bytes=492\n\t\tSpilled Records=0\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=0\n\t\tGC time elapsed (ms)=8620\n\t\tCPU time spent (ms)=231430\n\t\tPhysical memory (bytes) snapshot=1745022976\n\t\tVirtual memory (bytes) snapshot=11496931328\n\t\tTotal committed heap usage (bytes)=994050048\n\t\tPeak Map Physical memory (bytes)=470241280\n\t\tPeak Map Virtual memory (bytes)=2878021632\n\tFile Input Format Counters \n\t\tBytes Read=0\n\tFile Output Format Counters \n\t\tBytes Written=0\n19/05/19 22:54:10 INFO mapreduce.ImportJobBase: Transferred 36.2073 MB in 224.7081 seconds (164.9974 KB/sec)\n19/05/19 22:54:10 INFO mapreduce.ImportJobBase: Retrieved 2595732 records.\n"}]},"apps":[],"jobName":"paragraph_1557693101636_-1595292897","id":"20190512-203141_2003549869","dateCreated":"2019-05-12T20:31:41+0000","dateStarted":"2019-05-19T22:49:59+0000","dateFinished":"2019-05-19T22:54:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3018"},{"text":"%md\n#### Step 2: Read into a Spark Dataframe\n\n##### ---------------------------------------------------------------------------------------------\n","user":"anonymous","dateUpdated":"2019-05-22T22:57:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Step 2: Read into a Spark Dataframe</h4>\n<h5>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</h5>\n"}]},"apps":[],"jobName":"paragraph_1558307438331_-2110078250","id":"20190519-231038_1109051946","dateCreated":"2019-05-19T23:10:38+0000","dateStarted":"2019-05-22T22:57:15+0000","dateFinished":"2019-05-21T22:22:12+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:3019"},{"text":"%spark2.pyspark\n\n##### NOTES #####\n# read in the new parquet files into Spark\ndf = spark.read.parquet('hdfs:///user/zeppelin/sqoopFull/*.parquet')\n\n# saw someone else do this, seem like it could save some time if I need to undo something note: this does not do what you want\ndf1 = df\n\n# required step (if only for readability)\ndf1 = df1.select(['household_key','BASKET_ID','SALES_VALUE','DAY','QUANTITY'])\n\n# It was decided to merge \"probable homeowners\" into \"homeowners\"\nreplace_df = df.na.replace('Probable Owner', 'Homeowner', 'HOMEOWNER_DESC')\nhomeowner_df = replace_df.where(replace_df['HOMEOWNER_DESC'] == 'Homeowner')\nhomeowner_df = homeowner_df.select(['household_key','BASKET_ID','SALES_VALUE','DAY','QUANTITY'])\n\n","user":"anonymous","dateUpdated":"2019-06-10T13:56:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558306804811_1910836080","id":"20190519-230004_1511403969","dateCreated":"2019-05-19T23:00:04+0000","dateStarted":"2019-05-27T20:48:52+0000","dateFinished":"2019-05-27T20:50:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3020"},{"text":"%spark2.pyspark\n# note for me: this first way won't work with calling the pyspark.sql.functions countDistinct AND the standard sum/min'\n# you must import functions and use at least the countDistinct function from it; the standard count will work programmatically, but will not\n# give a true count of shopping visits \n# df1_agg = df1.groupBy(df1['household_key']).agg({'DAY':'min','BASKET_ID':'count', 'SALES_VALUE':'sum'}).withColumnRenamed('min(DAY)', 'recency')\\\n#   .withColumnRenamed('count(BASKET_ID)','frequency').withColumnRenamed('sum(SALES_VALUE)', 'monetary_value')\n\nimport pyspark.sql.functions as F\nall_agg = df1.groupBy('household_key').agg(F.min('DAY'),F.countDistinct('BASKET_ID'),F.sum('SALES_VALUE')).withColumnRenamed('min(DAY)', 'recency')\\\n    .withColumnRenamed('count(DISTINCT BASKET_ID)','frequency').withColumnRenamed('sum(SALES_VALUE)', 'monetary_value')\n    \n# separated for readability\nall_agg_order = all_agg.orderBy(['recency','frequency', 'monetary_value'], ascending=[1,0,1])\n\n# to csv\nall_agg_order.coalesce(1).write.csv('all_households_rfm')","user":"anonymous","dateUpdated":"2019-05-28T00:31:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558994436550_-466331186","id":"20190527-220036_780143424","dateCreated":"2019-05-27T22:00:36+0000","dateStarted":"2019-05-28T00:31:10+0000","dateFinished":"2019-05-28T00:31:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3021"},{"text":"%spark2.pyspark\n# import isn't needed twice but left as a convenience if one were to come back and only need 1 \nimport pyspark.sql.functions as F\n\nhomeowner_agg = homeowner_df.groupBy('household_key').agg(F.min('DAY'),F.countDistinct('BASKET_ID'),F.sum('SALES_VALUE')).withColumnRenamed('min(DAY)', 'recency')\\\n    .withColumnRenamed('count(DISTINCT BASKET_ID)','frequency').withColumnRenamed('sum(SALES_VALUE)', 'monetary_value')\n\n#separated for readibility\nhomeowner_agg_order = homeowner_agg.orderBy(['recency','frequency', 'monetary_value'], ascending=[1,0,1])\n\n# to csv\n# homeowner_agg_order.write.csv('file:///data/homeowner_rfm.csv') #run down later why there were ~20 partitions\nhomeowner_agg_order.coalesce(1).write.csv('homeowner_rfm.csv')","user":"anonymous","dateUpdated":"2019-05-28T00:30:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558995388434_-954065595","id":"20190527-221628_21127159","dateCreated":"2019-05-27T22:16:28+0000","dateStarted":"2019-05-28T00:12:12+0000","dateFinished":"2019-05-28T00:13:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3022"},{"text":"%md\n\n### Below is the record of my explorations and fails - I will keep them in this notebook for future reference","user":"anonymous","dateUpdated":"2019-05-27T23:52:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Below is the record of my explorations and fails - I will keep them in this notebook for future reference</h3>\n"}]},"apps":[],"jobName":"paragraph_1558306742781_-1140695588","id":"20190519-225902_1229777563","dateCreated":"2019-05-19T22:59:02+0000","dateStarted":"2019-05-27T23:52:44+0000","dateFinished":"2019-05-27T23:52:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3023"},{"text":"%spark2.pyspark\nhomeowner_agg = homeowner_df.groupBy(homeowner_df['household_key']).agg({'DAY':'min','BASKET_ID':'count', 'SALES_VALUE':'sum'}).withColumnRenamed('min(DAY)', 'recency')\\\n    .withColumnRenamed('count(BASKET_ID)','frequency').withColumnRenamed('sum(SALES_VALUE)', 'monetary_value')\nhomeowner_agg.show()\n\n#df1.sort('DAY').show(4)\ndf_day = df1.groupBy('household_key').agg({'DAY':'min'})\nprint(df_day.select('household_key').distinct().count())\ndf_day.count()\n%spark2.pyspark\ndf1.groupBy('household_key').agg({'BASKET_ID':'count'}).orderBy('count(BASKET_ID)', ascending=False).show()\n%spark2.pyspark\nhousehold = df1.groupBy(df1['household_key']).agg({'SALES_VALUE':'sum'}).orderBy('sum(SALES_VALUE)',ascending=False)\nhousehold.show(3)\n\n%spark2.pyspark\n\n# I will need to aggregate on basket_id whilst getting the sum for all the items in the basket which \n# involves multiplying quantity by sales_value\n# df.withColumn('age2', df.age + 2).collect() unneeded as sales value seems to be the sales value, not the item value\n# df1.withColumn()\n#group_by_dataframe.count().filter(\"`count` >= 10\").orderBy('count', ascending=False)\n#df1.groupBy(\"BASKET_ID\")\n#df1.describe().show()\n#df1.crosstab('BASKET_ID', 'SALES_VALUE').show()\n#df.select('household_key').count()\n#df.distinct().count() - memory insufficient\n#df.count()\n#df1.columns\n\n#so I will need household_key, basket_id to count them, sales value to add them, and DAY for recency\n# do I need quantity? yes as basket's items are listed in several rows\n\n#df1 = df1.select('household_key', 'BASKET_ID', 'QUANTITY', 'SALES_VALUE')\n#print(df.count())\n#df.where(df['BASKET_ID'].isNotNull()).select('BASKET_ID').count()\n#df.where(df['household_key'].isNull()).count()\n# - still surprised there are no nulls - seems it's just not seeing them\n\n#df.groupby('BASKET_ID').count().show()","user":"anonymous","dateUpdated":"2019-05-27T23:28:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558996357969_-2075598435","id":"20190527-223237_2054134820","dateCreated":"2019-05-27T22:32:37+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3024"},{"text":"%spark2.pyspark\nbasket_sum = df1.groupBy(df1['BASKET_ID']).agg({'SALES_VALUE':'sum'}).orderBy('sum(SALES_VALUE)', ascending=False)\nbasket_sum.show()","user":"anonymous","dateUpdated":"2019-05-27T21:29:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+------------------+\n|  BASKET_ID|  sum(SALES_VALUE)|\n+-----------+------------------+\n|32006114302|            961.49|\n|40387571385| 681.5799999999999|\n|33347880492| 552.9200000000003|\n|32505140858|            545.17|\n|30983766334| 545.0699999999999|\n|35638473903|            543.83|\n|34267311742| 518.0100000000001|\n|30515165970|            508.58|\n|32187143334|            499.99|\n|41560297395|497.02000000000004|\n|32932343176| 472.8700000000001|\n|33655186256| 471.7800000000001|\n|31242648741| 470.7400000000001|\n|27534431747|465.70000000000016|\n|32556306682|463.97000000000037|\n|27798435271|463.16999999999996|\n|30031850855|            456.79|\n|32075020200| 454.8000000000003|\n|30662484140|447.16000000000014|\n|29712930072|437.96999999999997|\n+-----------+------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558992516000_730951757","id":"20190527-212836_516862623","dateCreated":"2019-05-27T21:28:36+0000","dateStarted":"2019-05-27T21:29:28+0000","dateFinished":"2019-05-27T21:29:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3025"},{"text":"%sh\n\n##### NOTES #####\n# used to list tables to ensure I have their spelling correct\n# unfortunately there seems to not be a good way to use sqoop to list columns\n\nsqoop list-tables \\\n--username bigdata \\\n--password \"\" \\\n--driver com.microsoft.sqlserver.jdbc.SQLServerDriver \\\n--connect \"jdbc:sqlserver://bigdata220.database.windows.net:1433;database=week3\"","user":"anonymous","dateUpdated":"2019-06-12T14:54:03+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Warning: /usr/hdp/3.0.1.0-187/accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.1.0-187/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n19/05/19 22:19:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7.3.0.1.0-187\n19/05/19 22:19:08 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n19/05/19 22:19:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n19/05/19 22:19:08 INFO manager.SqlManager: Using default fetchSize of 1000\ncampaign_desc\ncampaign_table\ncausal_data\ncoupon\ncoupon_redempt\nhh_demographic\nproduct\ntransaction_data\ntrace_xe_action_map\ntrace_xe_event_map\n"}]},"apps":[],"jobName":"paragraph_1558304134820_-555808619","id":"20190519-221534_230470044","dateCreated":"2019-05-19T22:15:34+0000","dateStarted":"2019-05-19T22:18:58+0000","dateFinished":"2019-05-19T22:19:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3026"},{"text":"%sh\n\n##### NOTES #####\n# this is the easiest way I've found to use the zeppelin user context to -rm -r dirs\n\nhdfs dfs -rm -r /tmp/sqoopFull","user":"anonymous","dateUpdated":"2019-05-28T00:37:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"19/05/19 22:52:36 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox-hdp.hortonworks.com:8020/tmp/sqoopFull' to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/sqoopFull\n"}]},"apps":[],"jobName":"paragraph_1558306309520_-2001984980","id":"20190519-225149_487676027","dateCreated":"2019-05-19T22:51:49+0000","dateStarted":"2019-05-19T22:52:31+0000","dateFinished":"2019-05-19T22:52:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3027"},{"text":"%spark2.pyspark\n\n##### NOTES #####\n# doing what I know to explore the data\n# Sqoop seems to not give you the schema, no headers; looking at the data \n# from reading the (same) tables in the following paragraphs, from the parquet files, I see that\n# parquet files bring in the schema/headers\n\njdbcDF_hh = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://bigdata220.database.windows.net:1433;database=week3\") \\\n    .option(\"dbtable\", \"hh_demographic\") \\\n    .option(\"user\", \"bigdata\") \\\n    .option(\"password\", \"\") \\\n    .load()\n    \njdbcDF_td = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://bigdata220.database.windows.net:1433;database=week3\") \\\n    .option(\"dbtable\", \"transaction_data\") \\\n    .option(\"user\", \"bigdata\") \\\n    .option(\"password\", \"\"\") \\\n    .load()\n    \njdbcDF_p = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://bigdata220.database.windows.net:1433;database=week3\") \\\n    .option(\"dbtable\", \"product\") \\\n    .option(\"user\", \"bigdata\") \\\n    .option(\"password\", \"\") \\\n    .load()\n\njdbcDF_hh.printSchema()\njdbcDF_td.printSchema()\njdbcDF_p.printSchema()","user":"anonymous","dateUpdated":"2019-06-12T14:54:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- AGE_DESC: string (nullable = true)\n |-- MARITAL_STATUS_CODE: string (nullable = true)\n |-- INCOME_DESC: string (nullable = true)\n |-- HOMEOWNER_DESC: string (nullable = true)\n |-- HH_COMP_DESC: string (nullable = true)\n |-- HOUSEHOLD_SIZE_DESC: string (nullable = true)\n |-- KID_CATEGORY_DESC: string (nullable = true)\n |-- household_key: integer (nullable = true)\n\nroot\n |-- household_key: integer (nullable = true)\n |-- BASKET_ID: long (nullable = true)\n |-- DAY: integer (nullable = true)\n |-- PRODUCT_ID: integer (nullable = true)\n |-- QUANTITY: integer (nullable = true)\n |-- SALES_VALUE: double (nullable = true)\n |-- STORE_ID: integer (nullable = true)\n |-- RETAIL_DISC: double (nullable = true)\n |-- TRANS_TIME: integer (nullable = true)\n |-- WEEK_NO: integer (nullable = true)\n |-- COUPON_DISC: double (nullable = true)\n |-- COUPON_MATCH_DISC: double (nullable = true)\n\nroot\n |-- PRODUCT_ID: integer (nullable = true)\n |-- MANUFACTURER: integer (nullable = true)\n |-- DEPARTMENT: string (nullable = true)\n |-- BRAND: string (nullable = true)\n |-- COMMODITY_DESC: string (nullable = true)\n |-- SUB_COMMODITY_DESC: string (nullable = true)\n |-- CURR_SIZE_OF_PRODUCT: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1557699066797_-938146460","id":"20190512-221106_1847052577","dateCreated":"2019-05-12T22:11:06+0000","dateStarted":"2019-05-22T02:09:20+0000","dateFinished":"2019-05-22T02:09:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3028"},{"text":"%spark2.pyspark\n\n# yes, I need quantity also\n\ndf1.select(['household_key','QUANTITY','BASKET_ID','SALES_VALUE']).show()","user":"anonymous","dateUpdated":"2019-05-28T00:37:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+-----------+-----------+\n|household_key|QUANTITY|  BASKET_ID|SALES_VALUE|\n+-------------+--------+-----------+-----------+\n|         1876|       1|27871126221|       1.19|\n|         1876|       1|27871126221|       2.61|\n|         1876|       1|27871126221|      29.34|\n|         1876|       1|27871126221|       0.99|\n|         1876|       1|27871126221|       1.29|\n|         1876|       1|41704686958|       3.34|\n|         1876|       1|41704686958|       1.29|\n|         1876|       1|41834837944|        1.2|\n|         1876|       2|41834837944|       7.08|\n|         1876|       1|41834837944|        3.0|\n|         1876|       1|41834837944|       1.88|\n|         1876|   13814|41865541684|       29.0|\n|         1876|       1|42198662893|       1.99|\n|         1876|       1|42198662893|       1.99|\n|         1877|       1|27353216874|       1.59|\n|         1877|       1|27353216874|       1.88|\n|         1877|       2|27353217908|        0.6|\n|         1877|       1|27442513144|       3.49|\n|         1877|       1|27443083252|       1.29|\n|         1877|       1|27443083252|       0.34|\n+-------------+--------+-----------+-----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558489351316_-2034734661","id":"20190522-014231_1763361827","dateCreated":"2019-05-22T01:42:31+0000","dateStarted":"2019-05-22T01:46:29+0000","dateFinished":"2019-05-22T01:46:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3029"},{"text":"%spark2.pyspark\n#jdbcDF_td.select(['household_key','BASKET_ID','SALES_VALUE','TRANS_TIME','DAY']).show()\n#jdbcDF_hh.select('HOMEOWNER_DESC').distinct().show()\n#jdbcDF_hh.groupby('HOMEOWNER_DESC').withColumn()\n\nfrom pyspark.sql.functions import countDistinct\n#jdbcDF_hh.groupBy(\"HOMEOWNER_DESC\").agg((\"HOMEOWNER_DESC\")).show()\njdbcDF_hh.groupBy(\"HOMEOWNER_DESC\").count().show()\n\n# should i combine Probable Renter and Renter/Homeowner/probable ??\n\njdbcDF_hh.","user":"anonymous","dateUpdated":"2019-05-22T01:57:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+-----+\n| HOMEOWNER_DESC|count|\n+---------------+-----+\n|Probable Renter|   11|\n|        Unknown|  233|\n|         Renter|   42|\n| Probable Owner|   11|\n|      Homeowner|  504|\n+---------------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1558487554950_1189884248","id":"20190522-011234_150554470","dateCreated":"2019-05-22T01:12:34+0000","dateStarted":"2019-05-22T01:40:20+0000","dateFinished":"2019-05-22T01:40:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3030"},{"text":"%spark2.pyspark\n#jdbcDF_hh.groupBy(\"HOMEOWNER_DESC\").count().show()\n\n# should i combine Probable Renter and Renter/Homeowner/probable ??\n\nreplace_df = jdbcDF_hh.na.replace('Probable Owner', 'Homeowner', 'HOMEOWNER_DESC')\nreplace_df.groupBy(\"HOMEOWNER_DESC\").count().show()","user":"anonymous","dateUpdated":"2019-05-22T02:09:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+-----+\n| HOMEOWNER_DESC|count|\n+---------------+-----+\n|Probable Renter|   11|\n|        Unknown|  233|\n|         Renter|   42|\n|      Homeowner|  515|\n+---------------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1558490256735_291063497","id":"20190522-015736_837602061","dateCreated":"2019-05-22T01:57:36+0000","dateStarted":"2019-05-22T02:09:35+0000","dateFinished":"2019-05-22T02:09:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3031"},{"text":"%spark2.pyspark\n#jdbcDF_hh, jbdcDF_td, jdbcDF_P\n# I thought I had seen count for the transaction data df was 97k, then I saw \n# the df from joining the 3 tables was 2.5m I thought something went terribly wrong \n# with the joins. I guess I was mistaken and if the isNotNull below is working there \n# are no household_key nulls\n\nfrom pyspark.sql.functions import isnan, when, count, col\n\n\ndfc = jdbcDF_td.where(jdbcDF_td['COUPON_MATCH_DISC'].isNotNull()).select('COUPON_MATCH_DISC')\n\nprint(dfc.count())\nprint(jdbcDF_td.count())","user":"anonymous","dateUpdated":"2019-05-21T02:22:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2595732\n2595732\n"}]},"apps":[],"jobName":"paragraph_1558402173830_-1698951802","id":"20190521-012933_26920637","dateCreated":"2019-05-21T01:29:33+0000","dateStarted":"2019-05-21T02:22:16+0000","dateFinished":"2019-05-21T02:23:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3032"},{"text":"%spark2.pyspark\n\n##### NOTES #####\n# an effort to explore the data; with manually created schema\n\nfrom pyspark.sql.types import StructType, StringType, IntegerType, StructField\n\nschema = StructType([\n    StructField(\"AGE_DESC\", StringType()),\n    StructField(\"MARITAL_STATUS_CODE\", StringType()),\n    StructField(\"INCOME_DESC\", StringType()),\n    StructField(\"HOMEOWNER_DESC\", StringType()),\n    StructField(\"HH_COMP_DESC\", StringType()),\n    StructField(\"HOUSEHOLD_SIZE_DESC\", StringType()),\n    StructField(\"KID_CATEGORY_DESC\", StringType()),\n    StructField(\"household_key\", IntegerType())])\n\ndf_hh = spark.read.schema(schema).parquet('hdfs:///user/root/hh_demographic/*.parquet')\ndf_hh.show()","user":"anonymous","dateUpdated":"2019-05-21T23:47:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.NullPointerException\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:209)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:165)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:407)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1557757308656_2141740414","id":"20190513-142148_1374638786","dateCreated":"2019-05-13T14:21:48+0000","dateStarted":"2019-05-21T23:47:42+0000","dateFinished":"2019-05-21T23:47:42+0000","status":"ERROR","errorMessage":"java.lang.NullPointerException\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:209)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:165)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:407)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n","progressUpdateIntervalMs":500,"$$hashKey":"object:3033"},{"text":"%spark2.pyspark\n# an effort to explore the data; with manually created schema. Hope it's right!\n\nfrom pyspark.sql.types import StructType, StringType, IntegerType, LongType, DoubleType, StructField\n\nschema = StructType([\n    StructField(\"household_key\", IntegerType()),\n    StructField(\"BASKET_ID\", LongType()),\n    StructField(\"DAY\", IntegerType()),\n    StructField(\"PRODUCT_ID\", IntegerType()),\n    StructField(\"QUANTITY\", IntegerType()),\n    StructField(\"SALES_VALUE\", DoubleType()),\n    StructField(\"STORE_ID\", IntegerType()),\n    StructField(\"RETAIL_DISC\", DoubleType()),\n    StructField(\"TRANS_TIME\", IntegerType()),\n    StructField(\"WEEK_NO\", IntegerType()),\n    StructField(\"COUPON_DISC\", DoubleType()),\n    StructField(\"COUPON_MATCH_DISC\", DoubleType())])\n    \n\n\ndf_td = spark.read.schema(schema).parquet('hdfs:///user/root/transaction_data/*.parquet')\ndf_td.show()","user":"anonymous","dateUpdated":"2019-05-21T01:33:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-----------+---+----------+--------+-----------+--------+-----------+----------+-------+-----------+-----------------+\n|household_key|  BASKET_ID|DAY|PRODUCT_ID|QUANTITY|SALES_VALUE|STORE_ID|RETAIL_DISC|TRANS_TIME|WEEK_NO|COUPON_DISC|COUPON_MATCH_DISC|\n+-------------+-----------+---+----------+--------+-----------+--------+-----------+----------+-------+-----------+-----------------+\n|            1|30930406468|263|    843559|       1|        2.0|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    856942|       1|       2.99|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    871570|       1|        2.5|     436|      -0.99|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    888996|       1|       1.69|     436|       -0.2|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    928891|       1|       1.79|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    938004|       2|       1.98|     436|       -0.6|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    940839|       1|       2.49|     436|       -0.3|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    951593|       1|       1.99|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    987635|       1|       3.99|     436|       -2.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|    995242|       1|       1.89|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1005274|       1|       1.98|     436|      -1.71|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1006184|       1|       2.99|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1013167|       1|       3.99|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1043064|       2|       1.38|     436|       -0.6|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1056509|       1|       1.97|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1082185|       1|       0.72|     436|       -0.5|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1084555|       1|       3.29|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1109465|       1|       2.69|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1111870|       1|       2.89|     436|        0.0|      1458|     38|        0.0|              0.0|\n|            1|30930406468|263|   1124029|       1|       3.99|     436|        0.0|      1458|     38|        0.0|              0.0|\n+-------------+-----------+---+----------+--------+-----------+--------+-----------+----------+-------+-----------+-----------------+\nonly showing top 20 rows\n\nroot\n |-- household_key: integer (nullable = true)\n |-- BASKET_ID: long (nullable = true)\n |-- DAY: integer (nullable = true)\n |-- PRODUCT_ID: integer (nullable = true)\n |-- QUANTITY: integer (nullable = true)\n |-- SALES_VALUE: double (nullable = true)\n |-- STORE_ID: integer (nullable = true)\n |-- RETAIL_DISC: double (nullable = true)\n |-- TRANS_TIME: integer (nullable = true)\n |-- WEEK_NO: integer (nullable = true)\n |-- COUPON_DISC: double (nullable = true)\n |-- COUPON_MATCH_DISC: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1557792254967_1529988306","id":"20190514-000414_53788312","dateCreated":"2019-05-14T00:04:14+0000","dateStarted":"2019-05-14T14:54:45+0000","dateFinished":"2019-05-14T14:54:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3034"},{"text":"%spark2.pyspark\n\n# an effort to explore the data; with manually created schema. Hope it's right!\n\nfrom pyspark.sql.types import StructType, StringType, IntegerType, LongType, DoubleType, StructField\n\nschema = StructType([\n    StructField(\"PRODUCT_ID\", IntegerType()),\n    StructField(\"MANUFACTURER\", IntegerType()),\n    StructField(\"DEPARTMENT\", StringType()),\n    StructField(\"BRAND\", StringType()),\n    StructField(\"COMMODITY_DESC\", StringType()),\n    StructField(\"SUB_COMMODITY_DESC\", StringType()),\n    StructField(\"CURR_SIZE_OF_PRODUCT\", StringType())])\n\ndf_p = spark.read.parquet('hdfs:///user/root/product/*.parquet')\ndf_p.show()","user":"anonymous","dateUpdated":"2019-05-21T01:33:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------+------------+--------+--------------------+--------------------+--------------------+\n|PRODUCT_ID|MANUFACTURER|  DEPARTMENT|   BRAND|      COMMODITY_DESC|  SUB_COMMODITY_DESC|CURR_SIZE_OF_PRODUCT|\n+----------+------------+------------+--------+--------------------+--------------------+--------------------+\n|     25671|           2|     GROCERY|National|            FRZN ICE| ICE - CRUSHED/CUBED|               22 LB|\n|     26081|           2|MISC. TRANS.|National|NO COMMODITY DESC...|NO SUBCOMMODITY D...|                    |\n|     26093|          69|      PASTRY| Private|               BREAD|BREAD:ITALIAN/FRENCH|                    |\n|     26190|          69|     GROCERY| Private|FRUIT - SHELF STABLE|         APPLE SAUCE|               50 OZ|\n|     26355|          69|     GROCERY| Private|       COOKIES/CONES|   SPECIALTY COOKIES|               14 OZ|\n|     26426|          69|     GROCERY| Private|   SPICES & EXTRACTS| SPICES & SEASONINGS|              2.5 OZ|\n|     26540|          69|     GROCERY| Private|       COOKIES/CONES|TRAY PACK/CHOC CH...|               16 OZ|\n|     26601|          69|     DRUG GM| Private|            VITAMINS|  VITAMIN - MINERALS|            300CT(1)|\n|     26636|          69|      PASTRY| Private|    BREAKFAST SWEETS|SW GDS: SW ROLLS/DAN|                    |\n|     26691|          16|     GROCERY| Private|  PNT BTR/JELLY/JAMS|               HONEY|               12 OZ|\n|     26738|          69|     GROCERY| Private|ICE CREAM/MILK/SH...|         TRADITIONAL|               56 OZ|\n|     26889|          32|     DRUG GM|National|            MAGAZINE|   TV/MOVIE-MAGAZINE|                    |\n|     26941|          69|     GROCERY| Private|ICE CREAM/MILK/SH...|         TRADITIONAL|               56 OZ|\n|     27021|           2|     GROCERY|National|            AIR CARE| AIR CARE - AEROSOLS|                    |\n|     27030|          69|     GROCERY| Private|ICE CREAM/MILK/SH...|         TRADITIONAL|               56 OZ|\n|     27152|          69|     GROCERY| Private|   SPICES & EXTRACTS| SPICES & SEASONINGS|                4 OZ|\n|     27158|          69|     GROCERY| Private|ICE CREAM/MILK/SH...|         TRADITIONAL|               56 OZ|\n|     27159|          69|     GROCERY| Private|              CHEESE|       STRING CHEESE|                1 OZ|\n|     27160|          69|     GROCERY| Private|      SHORTENING/OIL| VEGETABLE/SALAD OIL|               48 OZ|\n|     27323|          69|     GROCERY| Private|              COFFEE|INSTANT DECAF FLV...|                9 OZ|\n+----------+------------+------------+--------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1557792819411_1028233894","id":"20190514-001339_1609712278","dateCreated":"2019-05-14T00:13:39+0000","dateStarted":"2019-05-14T15:00:39+0000","dateFinished":"2019-05-14T15:00:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3035"},{"text":"%spark2.pyspark\ndf_p.select('COMMODITY_DESC').distinct().show()","user":"anonymous","dateUpdated":"2019-05-21T01:33:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o285.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 19.0 failed 1 times, most recent failure: Lost task 1.0 in stage 19.0 (TID 20, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:107)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:108)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n\tat org.apache.spark.storage.DiskBlockObjectWriter$ManualCloseBufferedOutputStream$1.<init>(DiskBlockObjectWriter.scala:107)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:108)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o285.showString.\\n', JavaObject id=o286), <traceback object at 0x7f66ada6b248>)"}]},"apps":[],"jobName":"paragraph_1557846022370_-320924873","id":"20190514-150022_2076580017","dateCreated":"2019-05-14T15:00:22+0000","dateStarted":"2019-05-14T15:37:04+0000","dateFinished":"2019-05-14T15:37:13+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3036"},{"text":"%spark2.pyspark\n","user":"anonymous","dateUpdated":"2019-05-14T15:36:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1557848204917_-889957465","id":"20190514-153644_1832440914","dateCreated":"2019-05-14T15:36:44+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3037"}],"name":"Sqoop Assignment Hagan","id":"2EDBS8ZND","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}